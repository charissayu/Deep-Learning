{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of 0908_DL_RsN_(3)_2_2_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12239af7abb44162a8921bf54b9efa6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3cf5130020b44967ad8ceefe382278f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_19ed1b7dd15e44d989042aa4e574c92b",
              "IPY_MODEL_83473a86204846f6a6526a7a6b76df59"
            ]
          }
        },
        "3cf5130020b44967ad8ceefe382278f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19ed1b7dd15e44d989042aa4e574c92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6741050240e4d5ea93555bf79c6ea68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce264476d1fc4a3fbdbc846d3fbe01d0"
          }
        },
        "83473a86204846f6a6526a7a6b76df59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51ea5303555d4f81a5b0be7f58ed78d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 50087604.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f98b7aa318fa4c35a0a8877c66d5a48e"
          }
        },
        "e6741050240e4d5ea93555bf79c6ea68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce264476d1fc4a3fbdbc846d3fbe01d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51ea5303555d4f81a5b0be7f58ed78d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f98b7aa318fa4c35a0a8877c66d5a48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA4pKc3ca-li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split, SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfi0NiqstXjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "259d2c57-5d3f-4a5a-c741-ec3d80321d90"
      },
      "source": [
        "#Set cuda environment\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGYlHqQcOZ5x",
        "colab_type": "text"
      },
      "source": [
        "**Load data with data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLyPzrSgIFD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply data augmentation to data\n",
        "img_transforms0 = transforms.Compose([transforms.ColorJitter(brightness=0.5, contrast=0.2, saturation=0.2),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomRotation(degrees=15),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform0 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txv13EXUIFLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "12239af7abb44162a8921bf54b9efa6e",
            "3cf5130020b44967ad8ceefe382278f3",
            "19ed1b7dd15e44d989042aa4e574c92b",
            "83473a86204846f6a6526a7a6b76df59",
            "e6741050240e4d5ea93555bf79c6ea68",
            "ce264476d1fc4a3fbdbc846d3fbe01d0",
            "51ea5303555d4f81a5b0be7f58ed78d6",
            "f98b7aa318fa4c35a0a8877c66d5a48e"
          ]
        },
        "outputId": "daffc78c-33b8-483b-e636-8f8b0dec5529"
      },
      "source": [
        "dataset_tf0 = CIFAR10(root='data/', train=True, download=True, transform=img_transforms0)\n",
        "dataset_org0 = CIFAR10(root='data/', train=True, download=True, transform=transform0)\n",
        "\n",
        "dataset_size = len(dataset_tf0)\n",
        "dataset_indices = list(range(dataset_size))\n",
        "\n",
        "val_split_index = int(np.floor(0.2 * dataset_size))\n",
        "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "train_tf_loader0 = DataLoader(dataset_tf0, batch_size=64, shuffle=False, num_workers=4, sampler=train_sampler, pin_memory=True)\n",
        "val_tf_loader0 = DataLoader(dataset_org0, batch_size=64, shuffle=False, num_workers=4, sampler=val_sampler, pin_memory=True)\n",
        "\n",
        "test_dataset_0 = CIFAR10(root='data/', train=False, transform=transform0)\n",
        "test_loader_0 = DataLoader(test_dataset_0, batch_size = 64, num_workers=4, pin_memory=True)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12239af7abb44162a8921bf54b9efa6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECUW33Vva-mV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "8b963f8a-2871-45ba-9f4a-64f027d5fee4"
      },
      "source": [
        "img, label = dataset_tf0[0]\n",
        "img_shape = img.shape\n",
        "print('image shape', img_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image shape torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gz5ZfUja-mZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "90214d99-b967-40bb-afa6-ff6eab630635"
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "x = []\n",
        "for i in range(dataset_size):\n",
        "    x.append(dataset_tf0[i][1])\n",
        "uimg = torch.tensor(x).unique(sorted=True)\n",
        "uimg_count = torch.stack([(torch.tensor(x)==i).sum() for i in uimg])\n",
        "for i in range(len(uimg)):\n",
        "    print(f'{classes[i]}: {uimg_count[i].item()} count')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plane: 5000 count\n",
            "car: 5000 count\n",
            "bird: 5000 count\n",
            "cat: 5000 count\n",
            "deer: 5000 count\n",
            "dog: 5000 count\n",
            "frog: 5000 count\n",
            "horse: 5000 count\n",
            "ship: 5000 count\n",
            "truck: 5000 count\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ5_frHxP4SS",
        "colab_type": "text"
      },
      "source": [
        "**Load data without data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAmkt2_QP3NY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "5d714f9a-7bff-4ad9-aadf-876599a99170"
      },
      "source": [
        "#Load data without data augmentation\n",
        "transform2 = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "dataset_tf2 = CIFAR10(root='data/', train=True, download=True, transform=transform2)\n",
        "dataset_test = CIFAR10(root='data/', train=False, download=True, transform=transform2)\n",
        "\n",
        "dataset_size = len(dataset_tf2)\n",
        "dataset_indices = list(range(dataset_size))\n",
        "\n",
        "np.random.shuffle(dataset_indices)\n",
        "val_split_index = int(np.floor(0.2 * dataset_size))\n",
        "train_idx, val_idx = dataset_indices[val_split_index:], dataset_indices[:val_split_index]\n",
        "\n",
        "train_sampler2 = SubsetRandomSampler(train_idx)\n",
        "val_sampler2 = SubsetRandomSampler(val_idx)\n",
        "\n",
        "train_tf_loader2 = DataLoader(dataset_tf2, batch_size=64, shuffle=False, num_workers=4, sampler=train_sampler2, pin_memory=True)\n",
        "val_tf_loader2 = DataLoader(dataset_tf2, batch_size=64, shuffle=False, num_workers=4, sampler=val_sampler2, pin_memory=True)\n",
        "test_tf_loader2 = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeMqtF6srcsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define accuracy checking function\n",
        "def check_accuracy(loader, model):    \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "            \n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "        \n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}') "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhro1d0oa-no",
        "colab_type": "text"
      },
      "source": [
        "**1 Configure ResNet-18 from scratch**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTlMk4GMBqCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88yP7lQuPzM",
        "colab_type": "text"
      },
      "source": [
        "**1 ResNet 18 - build from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ntvxLxBv9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model1 = ResNet18()\n",
        "if torch.cuda.is_available():\n",
        "    model1.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model1.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGl54ivBrPqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "838f1cf7-2f4d-454a-aa16-3b6a83e8c379"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model1(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%1000 == 999: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/1000))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUFVTw3WPEhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model1.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tlzG6_5R0bZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2d5a23cf-ce7e-4921-b273-595fb3cde03d"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model1))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model1))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39729 / 40000 with accuracy 99.32\n",
            "train acc None\n",
            "Got 9016 / 10000 with accuracy 90.16\n",
            "val acc None\n",
            "Got 8974 / 10000 with accuracy 89.74\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52AOkdn1Xln",
        "colab_type": "text"
      },
      "source": [
        "**ResNet 34 Build from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYjM-yR1Wz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model1_34 = ResNet34()\n",
        "if torch.cuda.is_available():\n",
        "    model1_34.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model1_34.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1AQerJ11sLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa796c21-4794-48f1-a75e-a9d1ba76f4d6"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model1_34(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:1.921\n",
            "[1,   400] loss:1.597\n",
            "[1,   600] loss:1.439\n",
            "[2,   200] loss:1.262\n",
            "[2,   400] loss:1.125\n",
            "[2,   600] loss:1.057\n",
            "[3,   200] loss:0.933\n",
            "[3,   400] loss:0.890\n",
            "[3,   600] loss:0.849\n",
            "[4,   200] loss:0.787\n",
            "[4,   400] loss:0.765\n",
            "[4,   600] loss:0.747\n",
            "[5,   200] loss:0.675\n",
            "[5,   400] loss:0.695\n",
            "[5,   600] loss:0.641\n",
            "[6,   200] loss:0.627\n",
            "[6,   400] loss:0.589\n",
            "[6,   600] loss:0.585\n",
            "[7,   200] loss:0.559\n",
            "[7,   400] loss:0.565\n",
            "[7,   600] loss:0.558\n",
            "[8,   200] loss:0.515\n",
            "[8,   400] loss:0.524\n",
            "[8,   600] loss:0.500\n",
            "[9,   200] loss:0.479\n",
            "[9,   400] loss:0.464\n",
            "[9,   600] loss:0.474\n",
            "[10,   200] loss:0.436\n",
            "[10,   400] loss:0.443\n",
            "[10,   600] loss:0.448\n",
            "[11,   200] loss:0.394\n",
            "[11,   400] loss:0.418\n",
            "[11,   600] loss:0.416\n",
            "[12,   200] loss:0.371\n",
            "[12,   400] loss:0.385\n",
            "[12,   600] loss:0.391\n",
            "[13,   200] loss:0.345\n",
            "[13,   400] loss:0.364\n",
            "[13,   600] loss:0.362\n",
            "[14,   200] loss:0.322\n",
            "[14,   400] loss:0.341\n",
            "[14,   600] loss:0.346\n",
            "[15,   200] loss:0.304\n",
            "[15,   400] loss:0.328\n",
            "[15,   600] loss:0.327\n",
            "[16,   200] loss:0.296\n",
            "[16,   400] loss:0.296\n",
            "[16,   600] loss:0.306\n",
            "[17,   200] loss:0.278\n",
            "[17,   400] loss:0.288\n",
            "[17,   600] loss:0.294\n",
            "[18,   200] loss:0.256\n",
            "[18,   400] loss:0.276\n",
            "[18,   600] loss:0.280\n",
            "[19,   200] loss:0.239\n",
            "[19,   400] loss:0.249\n",
            "[19,   600] loss:0.266\n",
            "[20,   200] loss:0.230\n",
            "[20,   400] loss:0.246\n",
            "[20,   600] loss:0.253\n",
            "[21,   200] loss:0.224\n",
            "[21,   400] loss:0.240\n",
            "[21,   600] loss:0.222\n",
            "[22,   200] loss:0.216\n",
            "[22,   400] loss:0.229\n",
            "[22,   600] loss:0.230\n",
            "[23,   200] loss:0.196\n",
            "[23,   400] loss:0.207\n",
            "[23,   600] loss:0.211\n",
            "[24,   200] loss:0.183\n",
            "[24,   400] loss:0.193\n",
            "[24,   600] loss:0.195\n",
            "[25,   200] loss:0.178\n",
            "[25,   400] loss:0.198\n",
            "[25,   600] loss:0.203\n",
            "[26,   200] loss:0.167\n",
            "[26,   400] loss:0.173\n",
            "[26,   600] loss:0.189\n",
            "[27,   200] loss:0.164\n",
            "[27,   400] loss:0.158\n",
            "[27,   600] loss:0.178\n",
            "[28,   200] loss:0.151\n",
            "[28,   400] loss:0.157\n",
            "[28,   600] loss:0.166\n",
            "[29,   200] loss:0.146\n",
            "[29,   400] loss:0.152\n",
            "[29,   600] loss:0.166\n",
            "[30,   200] loss:0.136\n",
            "[30,   400] loss:0.150\n",
            "[30,   600] loss:0.150\n",
            "[31,   200] loss:0.130\n",
            "[31,   400] loss:0.138\n",
            "[31,   600] loss:0.140\n",
            "[32,   200] loss:0.124\n",
            "[32,   400] loss:0.142\n",
            "[32,   600] loss:0.130\n",
            "[33,   200] loss:0.121\n",
            "[33,   400] loss:0.136\n",
            "[33,   600] loss:0.133\n",
            "[34,   200] loss:0.107\n",
            "[34,   400] loss:0.131\n",
            "[34,   600] loss:0.135\n",
            "[35,   200] loss:0.108\n",
            "[35,   400] loss:0.119\n",
            "[35,   600] loss:0.124\n",
            "[36,   200] loss:0.105\n",
            "[36,   400] loss:0.116\n",
            "[36,   600] loss:0.120\n",
            "[37,   200] loss:0.102\n",
            "[37,   400] loss:0.119\n",
            "[37,   600] loss:0.113\n",
            "[38,   200] loss:0.093\n",
            "[38,   400] loss:0.099\n",
            "[38,   600] loss:0.102\n",
            "[39,   200] loss:0.094\n",
            "[39,   400] loss:0.103\n",
            "[39,   600] loss:0.105\n",
            "[40,   200] loss:0.087\n",
            "[40,   400] loss:0.103\n",
            "[40,   600] loss:0.096\n",
            "[41,   200] loss:0.090\n",
            "[41,   400] loss:0.090\n",
            "[41,   600] loss:0.109\n",
            "[42,   200] loss:0.089\n",
            "[42,   400] loss:0.090\n",
            "[42,   600] loss:0.099\n",
            "[43,   200] loss:0.083\n",
            "[43,   400] loss:0.092\n",
            "[43,   600] loss:0.092\n",
            "[44,   200] loss:0.078\n",
            "[44,   400] loss:0.087\n",
            "[44,   600] loss:0.086\n",
            "[45,   200] loss:0.082\n",
            "[45,   400] loss:0.088\n",
            "[45,   600] loss:0.095\n",
            "[46,   200] loss:0.079\n",
            "[46,   400] loss:0.077\n",
            "[46,   600] loss:0.088\n",
            "[47,   200] loss:0.070\n",
            "[47,   400] loss:0.076\n",
            "[47,   600] loss:0.087\n",
            "[48,   200] loss:0.066\n",
            "[48,   400] loss:0.068\n",
            "[48,   600] loss:0.077\n",
            "[49,   200] loss:0.076\n",
            "[49,   400] loss:0.066\n",
            "[49,   600] loss:0.082\n",
            "[50,   200] loss:0.067\n",
            "[50,   400] loss:0.072\n",
            "[50,   600] loss:0.066\n",
            "[51,   200] loss:0.067\n",
            "[51,   400] loss:0.066\n",
            "[51,   600] loss:0.069\n",
            "[52,   200] loss:0.061\n",
            "[52,   400] loss:0.062\n",
            "[52,   600] loss:0.073\n",
            "[53,   200] loss:0.054\n",
            "[53,   400] loss:0.068\n",
            "[53,   600] loss:0.069\n",
            "[54,   200] loss:0.063\n",
            "[54,   400] loss:0.063\n",
            "[54,   600] loss:0.068\n",
            "[55,   200] loss:0.056\n",
            "[55,   400] loss:0.067\n",
            "[55,   600] loss:0.062\n",
            "[56,   200] loss:0.062\n",
            "[56,   400] loss:0.057\n",
            "[56,   600] loss:0.071\n",
            "[57,   200] loss:0.061\n",
            "[57,   400] loss:0.061\n",
            "[57,   600] loss:0.058\n",
            "[58,   200] loss:0.058\n",
            "[58,   400] loss:0.062\n",
            "[58,   600] loss:0.062\n",
            "[59,   200] loss:0.048\n",
            "[59,   400] loss:0.051\n",
            "[59,   600] loss:0.057\n",
            "[60,   200] loss:0.059\n",
            "[60,   400] loss:0.050\n",
            "[60,   600] loss:0.062\n",
            "[61,   200] loss:0.056\n",
            "[61,   400] loss:0.052\n",
            "[61,   600] loss:0.057\n",
            "[62,   200] loss:0.055\n",
            "[62,   400] loss:0.056\n",
            "[62,   600] loss:0.047\n",
            "[63,   200] loss:0.052\n",
            "[63,   400] loss:0.046\n",
            "[63,   600] loss:0.051\n",
            "[64,   200] loss:0.049\n",
            "[64,   400] loss:0.051\n",
            "[64,   600] loss:0.044\n",
            "[65,   200] loss:0.045\n",
            "[65,   400] loss:0.053\n",
            "[65,   600] loss:0.053\n",
            "[66,   200] loss:0.047\n",
            "[66,   400] loss:0.051\n",
            "[66,   600] loss:0.045\n",
            "[67,   200] loss:0.043\n",
            "[67,   400] loss:0.053\n",
            "[67,   600] loss:0.051\n",
            "[68,   200] loss:0.037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JbEfBwf1ssS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model1_34.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPnXd-xw2iJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "51840a6f-27cf-4f9f-9c8e-c1c7bc868549"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model1_34))\n",
        "print('val acc', check_accuracy(val_tf_loader0, model1_34))\n",
        "print('test acc', check_accuracy(test_loader_0, model1_34))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39690 / 40000 with accuracy 99.22\n",
            "train acc None\n",
            "Got 9027 / 10000 with accuracy 90.27\n",
            "val acc None\n",
            "Got 8971 / 10000 with accuracy 89.71\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj-CvwhEUtTm",
        "colab_type": "text"
      },
      "source": [
        "**ResNet-50 build from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWWDHSG3XTD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model1_50 = ResNet50()\n",
        "if torch.cuda.is_available():\n",
        "    model1_50.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model1_50.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gckxlD2XarU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95854034-5e10-4a08-baea-16ea3f3d30c1"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model1_50(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "            \n",
        "print('finished', epoch)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:2.374\n",
            "[1,   400] loss:1.840\n",
            "[1,   600] loss:1.691\n",
            "[2,   200] loss:1.580\n",
            "[2,   400] loss:1.485\n",
            "[2,   600] loss:1.444\n",
            "[3,   200] loss:1.331\n",
            "[3,   400] loss:1.246\n",
            "[3,   600] loss:1.148\n",
            "[4,   200] loss:1.061\n",
            "[4,   400] loss:1.023\n",
            "[4,   600] loss:0.960\n",
            "[5,   200] loss:0.892\n",
            "[5,   400] loss:0.860\n",
            "[5,   600] loss:0.819\n",
            "[6,   200] loss:0.837\n",
            "[6,   400] loss:0.843\n",
            "[6,   600] loss:0.771\n",
            "[7,   200] loss:0.710\n",
            "[7,   400] loss:0.686\n",
            "[7,   600] loss:0.670\n",
            "[8,   200] loss:0.612\n",
            "[8,   400] loss:0.632\n",
            "[8,   600] loss:0.585\n",
            "[9,   200] loss:0.576\n",
            "[9,   400] loss:0.583\n",
            "[9,   600] loss:0.571\n",
            "[10,   200] loss:0.520\n",
            "[10,   400] loss:0.516\n",
            "[10,   600] loss:0.540\n",
            "[11,   200] loss:0.529\n",
            "[11,   400] loss:0.524\n",
            "[11,   600] loss:0.508\n",
            "[12,   200] loss:0.461\n",
            "[12,   400] loss:0.475\n",
            "[12,   600] loss:0.451\n",
            "[13,   200] loss:0.416\n",
            "[13,   400] loss:0.429\n",
            "[13,   600] loss:0.427\n",
            "[14,   200] loss:0.386\n",
            "[14,   400] loss:0.403\n",
            "[14,   600] loss:0.389\n",
            "[15,   200] loss:0.356\n",
            "[15,   400] loss:0.360\n",
            "[15,   600] loss:0.365\n",
            "[16,   200] loss:0.335\n",
            "[16,   400] loss:0.345\n",
            "[16,   600] loss:0.355\n",
            "[17,   200] loss:0.316\n",
            "[17,   400] loss:0.320\n",
            "[17,   600] loss:0.320\n",
            "[18,   200] loss:0.292\n",
            "[18,   400] loss:0.332\n",
            "[18,   600] loss:0.331\n",
            "[19,   200] loss:0.285\n",
            "[19,   400] loss:0.298\n",
            "[19,   600] loss:0.309\n",
            "[20,   200] loss:0.257\n",
            "[20,   400] loss:0.295\n",
            "[20,   600] loss:0.282\n",
            "[21,   200] loss:0.238\n",
            "[21,   400] loss:0.282\n",
            "[21,   600] loss:0.287\n",
            "[22,   200] loss:0.243\n",
            "[22,   400] loss:0.255\n",
            "[22,   600] loss:0.267\n",
            "[23,   200] loss:0.221\n",
            "[23,   400] loss:0.234\n",
            "[23,   600] loss:0.238\n",
            "[24,   200] loss:0.208\n",
            "[24,   400] loss:0.222\n",
            "[24,   600] loss:0.237\n",
            "[25,   200] loss:0.205\n",
            "[25,   400] loss:0.213\n",
            "[25,   600] loss:0.223\n",
            "[26,   200] loss:0.184\n",
            "[26,   400] loss:0.197\n",
            "[26,   600] loss:0.204\n",
            "[27,   200] loss:0.188\n",
            "[27,   400] loss:0.196\n",
            "[27,   600] loss:0.193\n",
            "[28,   200] loss:0.173\n",
            "[28,   400] loss:0.181\n",
            "[28,   600] loss:0.186\n",
            "[29,   200] loss:0.160\n",
            "[29,   400] loss:0.174\n",
            "[29,   600] loss:0.183\n",
            "[30,   200] loss:0.152\n",
            "[30,   400] loss:0.165\n",
            "[30,   600] loss:0.167\n",
            "[31,   200] loss:0.142\n",
            "[31,   400] loss:0.155\n",
            "[31,   600] loss:0.163\n",
            "[32,   200] loss:0.141\n",
            "[32,   400] loss:0.153\n",
            "[32,   600] loss:0.158\n",
            "[33,   200] loss:0.146\n",
            "[33,   400] loss:0.149\n",
            "[33,   600] loss:0.149\n",
            "[34,   200] loss:0.131\n",
            "[34,   400] loss:0.139\n",
            "[34,   600] loss:0.139\n",
            "[35,   200] loss:0.124\n",
            "[35,   400] loss:0.138\n",
            "[35,   600] loss:0.128\n",
            "[36,   200] loss:0.129\n",
            "[36,   400] loss:0.126\n",
            "[36,   600] loss:0.136\n",
            "[37,   200] loss:0.116\n",
            "[37,   400] loss:0.116\n",
            "[37,   600] loss:0.132\n",
            "[38,   200] loss:0.111\n",
            "[38,   400] loss:0.117\n",
            "[38,   600] loss:0.125\n",
            "[39,   200] loss:0.101\n",
            "[39,   400] loss:0.118\n",
            "[39,   600] loss:0.120\n",
            "[40,   200] loss:0.098\n",
            "[40,   400] loss:0.108\n",
            "[40,   600] loss:0.113\n",
            "[41,   200] loss:0.108\n",
            "[41,   400] loss:0.102\n",
            "[41,   600] loss:0.110\n",
            "[42,   200] loss:0.093\n",
            "[42,   400] loss:0.106\n",
            "[42,   600] loss:0.109\n",
            "[43,   200] loss:0.092\n",
            "[43,   400] loss:0.102\n",
            "[43,   600] loss:0.106\n",
            "[44,   200] loss:0.088\n",
            "[44,   400] loss:0.095\n",
            "[44,   600] loss:0.103\n",
            "[45,   200] loss:0.094\n",
            "[45,   400] loss:0.093\n",
            "[45,   600] loss:0.094\n",
            "[46,   200] loss:0.095\n",
            "[46,   400] loss:0.084\n",
            "[46,   600] loss:0.082\n",
            "[47,   200] loss:0.091\n",
            "[47,   400] loss:0.087\n",
            "[47,   600] loss:0.090\n",
            "[48,   200] loss:0.078\n",
            "[48,   400] loss:0.087\n",
            "[48,   600] loss:0.092\n",
            "[49,   200] loss:0.073\n",
            "[49,   400] loss:0.088\n",
            "[49,   600] loss:0.084\n",
            "[50,   200] loss:0.081\n",
            "[50,   400] loss:0.077\n",
            "[50,   600] loss:0.083\n",
            "[51,   200] loss:0.074\n",
            "[51,   400] loss:0.091\n",
            "[51,   600] loss:0.090\n",
            "[52,   200] loss:0.071\n",
            "[52,   400] loss:0.079\n",
            "[52,   600] loss:0.072\n",
            "[53,   200] loss:0.071\n",
            "[53,   400] loss:0.076\n",
            "[53,   600] loss:0.078\n",
            "[54,   200] loss:0.063\n",
            "[54,   400] loss:0.076\n",
            "[54,   600] loss:0.082\n",
            "[55,   200] loss:0.072\n",
            "[55,   400] loss:0.074\n",
            "[55,   600] loss:0.076\n",
            "[56,   200] loss:0.066\n",
            "[56,   400] loss:0.066\n",
            "[56,   600] loss:0.071\n",
            "[57,   200] loss:0.065\n",
            "[57,   400] loss:0.067\n",
            "[57,   600] loss:0.074\n",
            "[58,   200] loss:0.065\n",
            "[58,   400] loss:0.058\n",
            "[58,   600] loss:0.072\n",
            "[59,   200] loss:0.063\n",
            "[59,   400] loss:0.072\n",
            "[59,   600] loss:0.069\n",
            "[60,   200] loss:0.064\n",
            "[60,   400] loss:0.068\n",
            "[60,   600] loss:0.064\n",
            "[61,   200] loss:0.061\n",
            "[61,   400] loss:0.059\n",
            "[61,   600] loss:0.074\n",
            "[62,   200] loss:0.055\n",
            "[62,   400] loss:0.061\n",
            "[62,   600] loss:0.065\n",
            "[63,   200] loss:0.059\n",
            "[63,   400] loss:0.064\n",
            "[63,   600] loss:0.057\n",
            "[64,   200] loss:0.055\n",
            "[64,   400] loss:0.065\n",
            "[64,   600] loss:0.065\n",
            "[65,   200] loss:0.051\n",
            "[65,   400] loss:0.063\n",
            "[65,   600] loss:0.059\n",
            "[66,   200] loss:0.053\n",
            "[66,   400] loss:0.064\n",
            "[66,   600] loss:0.064\n",
            "[67,   200] loss:0.061\n",
            "[67,   400] loss:0.054\n",
            "[67,   600] loss:0.064\n",
            "[68,   200] loss:0.064\n",
            "[68,   400] loss:0.067\n",
            "[68,   600] loss:0.059\n",
            "[69,   200] loss:0.048\n",
            "[69,   400] loss:0.058\n",
            "[69,   600] loss:0.058\n",
            "[70,   200] loss:0.047\n",
            "[70,   400] loss:0.059\n",
            "[70,   600] loss:0.061\n",
            "[71,   200] loss:0.053\n",
            "[71,   400] loss:0.060\n",
            "[71,   600] loss:0.057\n",
            "[72,   200] loss:0.052\n",
            "[72,   400] loss:0.051\n",
            "[72,   600] loss:0.061\n",
            "[73,   200] loss:0.045\n",
            "[73,   400] loss:0.061\n",
            "[73,   600] loss:0.059\n",
            "[74,   200] loss:0.055\n",
            "[74,   400] loss:0.056\n",
            "[74,   600] loss:0.045\n",
            "[75,   200] loss:0.051\n",
            "[75,   400] loss:0.047\n",
            "[75,   600] loss:0.053\n",
            "[76,   200] loss:0.051\n",
            "[76,   400] loss:0.054\n",
            "[76,   600] loss:0.054\n",
            "[77,   200] loss:0.057\n",
            "[77,   400] loss:0.053\n",
            "[77,   600] loss:0.053\n",
            "[78,   200] loss:0.047\n",
            "[78,   400] loss:0.047\n",
            "[78,   600] loss:0.050\n",
            "[79,   200] loss:0.049\n",
            "[79,   400] loss:0.049\n",
            "[79,   600] loss:0.063\n",
            "[80,   200] loss:0.040\n",
            "[80,   400] loss:0.046\n",
            "[80,   600] loss:0.047\n",
            "finished 79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0PLCgYKXeqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model1_50.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxtK-YMNXiaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "359a05ee-37ed-4533-cbf8-ecba838b5ddf"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model1_50))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model1_50))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model1_50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39470 / 40000 with accuracy 98.67\n",
            "train acc None\n",
            "Got 8945 / 10000 with accuracy 89.45\n",
            "val acc None\n",
            "Got 8913 / 10000 with accuracy 89.13\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiMCseW23DSD",
        "colab_type": "text"
      },
      "source": [
        "**2. Compare with model training without Data Augmentation**\n",
        "\n",
        "**ResNet18 - train without data augmentation**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc1ngbyc3h1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model1_1 = ResNet18()\n",
        "if torch.cuda.is_available():\n",
        "    model1_1.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model1_1.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyOanLVo4Qur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f242c746-e803-4c54-b934-6b370ad663ab"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader2):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model1_1(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%1000 == 999: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/1000))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QXXl2Fx4xoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "bb1396c7-ba94-4295-c990-81a40f2aefed"
      },
      "source": [
        "print('train acc',check_accuracy(train_tf_loader2, model1_1))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader2, model1_1))\n",
        "\n",
        "print('test acc', check_accuracy(test_tf_loader2, model1_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 40000 / 40000 with accuracy 100.00\n",
            "train acc None\n",
            "Got 8611 / 10000 with accuracy 86.11\n",
            "val acc None\n",
            "Got 8484 / 10000 with accuracy 84.84\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I20pT505Spfq",
        "colab_type": "text"
      },
      "source": [
        "**2. ResNet 50 -Train without data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff3Pe-ejSoH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model2_2 = ResNet50()\n",
        "if torch.cuda.is_available():\n",
        "    model2_2.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model2_2.parameters(), lr=learning_rate,  momentum=0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFHEGz7OSoLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2644da3-c214-4c32-c457-1cd3661e95bb"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader2):\n",
        "        # Get data to cuda if possible\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model2_2(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "      \n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%1000 == 999: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/1000))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RqpEuB0SoOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "121b3a5d-2fa0-4f1d-d9e3-728227123d8f"
      },
      "source": [
        "print('train acc',check_accuracy(train_tf_loader2, model2_2))\n",
        "print('val acc', check_accuracy(val_tf_loader2, model2_2))\n",
        "print('test acc', check_accuracy(test_tf_loader2, model2_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39896 / 40000 with accuracy 99.74\n",
            "train acc None\n",
            "Got 8157 / 10000 with accuracy 81.57\n",
            "val acc None\n",
            "Got 8091 / 10000 with accuracy 80.91\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDDGqL4sgynT",
        "colab_type": "text"
      },
      "source": [
        "**3. Dropout ResNet 18 - 0.15**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwlWeWphOsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock_2(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock_2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.dropout(self.conv2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet_2(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_2, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        m = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "        out = m(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18_2():\n",
        "    return ResNet_2(BasicBlock_2, [2, 2, 2, 2])\n",
        "\n",
        "def ResNet34_2():\n",
        "    return ResNet_2(BasicBlock_2, [3, 4, 6, 3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIeFAlcDgZcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model8 = ResNet18_2()\n",
        "if torch.cuda.is_available():\n",
        "    model8.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model8.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiXyMh6TwAl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "500af52f-5b50-4860-9244-997a065e822a"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model8(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:1.924\n",
            "[1,   400] loss:1.578\n",
            "[1,   600] loss:1.427\n",
            "[2,   200] loss:1.269\n",
            "[2,   400] loss:1.154\n",
            "[2,   600] loss:1.093\n",
            "[3,   200] loss:0.969\n",
            "[3,   400] loss:0.942\n",
            "[3,   600] loss:0.893\n",
            "[4,   200] loss:0.809\n",
            "[4,   400] loss:0.804\n",
            "[4,   600] loss:0.777\n",
            "[5,   200] loss:0.717\n",
            "[5,   400] loss:0.701\n",
            "[5,   600] loss:0.692\n",
            "[6,   200] loss:0.654\n",
            "[6,   400] loss:0.644\n",
            "[6,   600] loss:0.621\n",
            "[7,   200] loss:0.602\n",
            "[7,   400] loss:0.596\n",
            "[7,   600] loss:0.581\n",
            "[8,   200] loss:0.531\n",
            "[8,   400] loss:0.556\n",
            "[8,   600] loss:0.548\n",
            "[9,   200] loss:0.500\n",
            "[9,   400] loss:0.510\n",
            "[9,   600] loss:0.513\n",
            "[10,   200] loss:0.467\n",
            "[10,   400] loss:0.486\n",
            "[10,   600] loss:0.464\n",
            "[11,   200] loss:0.444\n",
            "[11,   400] loss:0.438\n",
            "[11,   600] loss:0.452\n",
            "[12,   200] loss:0.421\n",
            "[12,   400] loss:0.430\n",
            "[12,   600] loss:0.418\n",
            "[13,   200] loss:0.381\n",
            "[13,   400] loss:0.412\n",
            "[13,   600] loss:0.402\n",
            "[14,   200] loss:0.361\n",
            "[14,   400] loss:0.378\n",
            "[14,   600] loss:0.398\n",
            "[15,   200] loss:0.338\n",
            "[15,   400] loss:0.358\n",
            "[15,   600] loss:0.355\n",
            "[16,   200] loss:0.331\n",
            "[16,   400] loss:0.355\n",
            "[16,   600] loss:0.347\n",
            "[17,   200] loss:0.305\n",
            "[17,   400] loss:0.319\n",
            "[17,   600] loss:0.333\n",
            "[18,   200] loss:0.291\n",
            "[18,   400] loss:0.309\n",
            "[18,   600] loss:0.316\n",
            "[19,   200] loss:0.281\n",
            "[19,   400] loss:0.280\n",
            "[19,   600] loss:0.290\n",
            "[20,   200] loss:0.277\n",
            "[20,   400] loss:0.280\n",
            "[20,   600] loss:0.280\n",
            "[21,   200] loss:0.263\n",
            "[21,   400] loss:0.259\n",
            "[21,   600] loss:0.262\n",
            "[22,   200] loss:0.234\n",
            "[22,   400] loss:0.250\n",
            "[22,   600] loss:0.265\n",
            "[23,   200] loss:0.237\n",
            "[23,   400] loss:0.231\n",
            "[23,   600] loss:0.260\n",
            "[24,   200] loss:0.213\n",
            "[24,   400] loss:0.225\n",
            "[24,   600] loss:0.239\n",
            "[25,   200] loss:0.201\n",
            "[25,   400] loss:0.221\n",
            "[25,   600] loss:0.223\n",
            "[26,   200] loss:0.204\n",
            "[26,   400] loss:0.215\n",
            "[26,   600] loss:0.213\n",
            "[27,   200] loss:0.193\n",
            "[27,   400] loss:0.212\n",
            "[27,   600] loss:0.207\n",
            "[28,   200] loss:0.174\n",
            "[28,   400] loss:0.185\n",
            "[28,   600] loss:0.208\n",
            "[29,   200] loss:0.175\n",
            "[29,   400] loss:0.182\n",
            "[29,   600] loss:0.191\n",
            "[30,   200] loss:0.169\n",
            "[30,   400] loss:0.192\n",
            "[30,   600] loss:0.188\n",
            "[31,   200] loss:0.156\n",
            "[31,   400] loss:0.164\n",
            "[31,   600] loss:0.184\n",
            "[32,   200] loss:0.149\n",
            "[32,   400] loss:0.157\n",
            "[32,   600] loss:0.166\n",
            "[33,   200] loss:0.149\n",
            "[33,   400] loss:0.155\n",
            "[33,   600] loss:0.158\n",
            "[34,   200] loss:0.145\n",
            "[34,   400] loss:0.161\n",
            "[34,   600] loss:0.142\n",
            "[35,   200] loss:0.130\n",
            "[35,   400] loss:0.140\n",
            "[35,   600] loss:0.148\n",
            "[36,   200] loss:0.133\n",
            "[36,   400] loss:0.138\n",
            "[36,   600] loss:0.144\n",
            "[37,   200] loss:0.114\n",
            "[37,   400] loss:0.133\n",
            "[37,   600] loss:0.145\n",
            "[38,   200] loss:0.115\n",
            "[38,   400] loss:0.123\n",
            "[38,   600] loss:0.133\n",
            "[39,   200] loss:0.122\n",
            "[39,   400] loss:0.124\n",
            "[39,   600] loss:0.124\n",
            "[40,   200] loss:0.114\n",
            "[40,   400] loss:0.118\n",
            "[40,   600] loss:0.129\n",
            "[41,   200] loss:0.110\n",
            "[41,   400] loss:0.110\n",
            "[41,   600] loss:0.116\n",
            "[42,   200] loss:0.101\n",
            "[42,   400] loss:0.116\n",
            "[42,   600] loss:0.114\n",
            "[43,   200] loss:0.111\n",
            "[43,   400] loss:0.117\n",
            "[43,   600] loss:0.114\n",
            "[44,   200] loss:0.099\n",
            "[44,   400] loss:0.107\n",
            "[44,   600] loss:0.109\n",
            "[45,   200] loss:0.098\n",
            "[45,   400] loss:0.099\n",
            "[45,   600] loss:0.110\n",
            "[46,   200] loss:0.090\n",
            "[46,   400] loss:0.093\n",
            "[46,   600] loss:0.099\n",
            "[47,   200] loss:0.094\n",
            "[47,   400] loss:0.104\n",
            "[47,   600] loss:0.101\n",
            "[48,   200] loss:0.090\n",
            "[48,   400] loss:0.098\n",
            "[48,   600] loss:0.097\n",
            "[49,   200] loss:0.081\n",
            "[49,   400] loss:0.091\n",
            "[49,   600] loss:0.097\n",
            "[50,   200] loss:0.082\n",
            "[50,   400] loss:0.086\n",
            "[50,   600] loss:0.081\n",
            "[51,   200] loss:0.076\n",
            "[51,   400] loss:0.090\n",
            "[51,   600] loss:0.093\n",
            "[52,   200] loss:0.080\n",
            "[52,   400] loss:0.077\n",
            "[52,   600] loss:0.083\n",
            "[53,   200] loss:0.074\n",
            "[53,   400] loss:0.085\n",
            "[53,   600] loss:0.095\n",
            "[54,   200] loss:0.076\n",
            "[54,   400] loss:0.074\n",
            "[54,   600] loss:0.086\n",
            "[55,   200] loss:0.068\n",
            "[55,   400] loss:0.068\n",
            "[55,   600] loss:0.081\n",
            "[56,   200] loss:0.072\n",
            "[56,   400] loss:0.070\n",
            "[56,   600] loss:0.090\n",
            "[57,   200] loss:0.072\n",
            "[57,   400] loss:0.078\n",
            "[57,   600] loss:0.077\n",
            "[58,   200] loss:0.067\n",
            "[58,   400] loss:0.075\n",
            "[58,   600] loss:0.079\n",
            "[59,   200] loss:0.067\n",
            "[59,   400] loss:0.065\n",
            "[59,   600] loss:0.068\n",
            "[60,   200] loss:0.071\n",
            "[60,   400] loss:0.060\n",
            "[60,   600] loss:0.075\n",
            "[61,   200] loss:0.065\n",
            "[61,   400] loss:0.074\n",
            "[61,   600] loss:0.076\n",
            "[62,   200] loss:0.058\n",
            "[62,   400] loss:0.070\n",
            "[62,   600] loss:0.063\n",
            "[63,   200] loss:0.052\n",
            "[63,   400] loss:0.067\n",
            "[63,   600] loss:0.074\n",
            "[64,   200] loss:0.059\n",
            "[64,   400] loss:0.055\n",
            "[64,   600] loss:0.047\n",
            "[65,   200] loss:0.057\n",
            "[65,   400] loss:0.061\n",
            "[65,   600] loss:0.061\n",
            "[66,   200] loss:0.057\n",
            "[66,   400] loss:0.061\n",
            "[66,   600] loss:0.065\n",
            "[67,   200] loss:0.055\n",
            "[67,   400] loss:0.057\n",
            "[67,   600] loss:0.063\n",
            "[68,   200] loss:0.053\n",
            "[68,   400] loss:0.056\n",
            "[68,   600] loss:0.060\n",
            "[69,   200] loss:0.056\n",
            "[69,   400] loss:0.051\n",
            "[69,   600] loss:0.062\n",
            "[70,   200] loss:0.058\n",
            "[70,   400] loss:0.055\n",
            "[70,   600] loss:0.058\n",
            "[71,   200] loss:0.058\n",
            "[71,   400] loss:0.060\n",
            "[71,   600] loss:0.054\n",
            "[72,   200] loss:0.058\n",
            "[72,   400] loss:0.057\n",
            "[72,   600] loss:0.058\n",
            "[73,   200] loss:0.063\n",
            "[73,   400] loss:0.051\n",
            "[73,   600] loss:0.062\n",
            "[74,   200] loss:0.056\n",
            "[74,   400] loss:0.044\n",
            "[74,   600] loss:0.055\n",
            "[75,   200] loss:0.047\n",
            "[75,   400] loss:0.044\n",
            "[75,   600] loss:0.059\n",
            "[76,   200] loss:0.045\n",
            "[76,   400] loss:0.051\n",
            "[76,   600] loss:0.054\n",
            "[77,   200] loss:0.050\n",
            "[77,   400] loss:0.048\n",
            "[77,   600] loss:0.046\n",
            "[78,   200] loss:0.044\n",
            "[78,   400] loss:0.046\n",
            "[78,   600] loss:0.046\n",
            "[79,   200] loss:0.044\n",
            "[79,   400] loss:0.051\n",
            "[79,   600] loss:0.053\n",
            "[80,   200] loss:0.049\n",
            "[80,   400] loss:0.046\n",
            "[80,   600] loss:0.058\n",
            "[81,   200] loss:0.046\n",
            "[81,   400] loss:0.047\n",
            "[81,   600] loss:0.053\n",
            "[82,   200] loss:0.038\n",
            "[82,   400] loss:0.052\n",
            "[82,   600] loss:0.051\n",
            "[83,   200] loss:0.043\n",
            "[83,   400] loss:0.043\n",
            "[83,   600] loss:0.051\n",
            "[84,   200] loss:0.043\n",
            "[84,   400] loss:0.045\n",
            "[84,   600] loss:0.051\n",
            "[85,   200] loss:0.040\n",
            "[85,   400] loss:0.040\n",
            "[85,   600] loss:0.042\n",
            "[86,   200] loss:0.040\n",
            "[86,   400] loss:0.046\n",
            "[86,   600] loss:0.051\n",
            "[87,   200] loss:0.041\n",
            "[87,   400] loss:0.041\n",
            "[87,   600] loss:0.042\n",
            "[88,   200] loss:0.043\n",
            "[88,   400] loss:0.047\n",
            "[88,   600] loss:0.044\n",
            "[89,   200] loss:0.040\n",
            "[89,   400] loss:0.044\n",
            "[89,   600] loss:0.048\n",
            "[90,   200] loss:0.043\n",
            "[90,   400] loss:0.045\n",
            "[90,   600] loss:0.047\n",
            "[91,   200] loss:0.042\n",
            "[91,   400] loss:0.044\n",
            "[91,   600] loss:0.048\n",
            "[92,   200] loss:0.041\n",
            "[92,   400] loss:0.039\n",
            "[92,   600] loss:0.045\n",
            "[93,   200] loss:0.039\n",
            "[93,   400] loss:0.041\n",
            "[93,   600] loss:0.040\n",
            "[94,   200] loss:0.037\n",
            "[94,   400] loss:0.038\n",
            "[94,   600] loss:0.044\n",
            "[95,   200] loss:0.039\n",
            "[95,   400] loss:0.040\n",
            "[95,   600] loss:0.043\n",
            "[96,   200] loss:0.038\n",
            "[96,   400] loss:0.040\n",
            "[96,   600] loss:0.039\n",
            "[97,   200] loss:0.039\n",
            "[97,   400] loss:0.043\n",
            "[97,   600] loss:0.042\n",
            "[98,   200] loss:0.036\n",
            "[98,   400] loss:0.034\n",
            "[98,   600] loss:0.037\n",
            "[99,   200] loss:0.034\n",
            "[99,   400] loss:0.032\n",
            "[99,   600] loss:0.047\n",
            "[100,   200] loss:0.038\n",
            "[100,   400] loss:0.043\n",
            "[100,   600] loss:0.043\n",
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTx-0Ie7F0QK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model8.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNNjq_rdmdHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "20968ae6-f5ff-4ebd-8501-3241563a739c"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model8))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model8))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model8))\n",
        "                                                                                 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39753 / 40000 with accuracy 99.38\n",
            "train acc None\n",
            "Got 9057 / 10000 with accuracy 90.57\n",
            "val acc None\n",
            "Got 9013 / 10000 with accuracy 90.13\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbMZtTz5PXyF",
        "colab_type": "text"
      },
      "source": [
        "**3 Dropout 0.15 ResNet 34**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwUeIXIGPCB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock_3(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock_3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(0.15)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.dropout(self.conv2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet_3(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_3, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        m = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "        out = m(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18_3():\n",
        "    return ResNet_3(BasicBlock_3, [2, 2, 2, 2])\n",
        "\n",
        "def ResNet34_3():\n",
        "    return ResNet_3(BasicBlock_3, [3, 4, 6, 3])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBY_kR-QPCFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model9 = ResNet34_3()\n",
        "if torch.cuda.is_available():\n",
        "    model9.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model9.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=0.0001)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDXNPi6IUoQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9047d953-39df-43e8-e540-04d6da26667f"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model9(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:2.014\n",
            "[1,   400] loss:1.642\n",
            "[1,   600] loss:1.497\n",
            "[2,   200] loss:1.350\n",
            "[2,   400] loss:1.254\n",
            "[2,   600] loss:1.113\n",
            "[3,   200] loss:1.046\n",
            "[3,   400] loss:0.969\n",
            "[3,   600] loss:0.918\n",
            "[4,   200] loss:0.870\n",
            "[4,   400] loss:0.847\n",
            "[4,   600] loss:0.786\n",
            "[5,   200] loss:0.748\n",
            "[5,   400] loss:0.732\n",
            "[5,   600] loss:0.721\n",
            "[6,   200] loss:0.666\n",
            "[6,   400] loss:0.675\n",
            "[6,   600] loss:0.636\n",
            "[7,   200] loss:0.629\n",
            "[7,   400] loss:0.624\n",
            "[7,   600] loss:0.607\n",
            "[8,   200] loss:0.578\n",
            "[8,   400] loss:0.569\n",
            "[8,   600] loss:0.559\n",
            "[9,   200] loss:0.528\n",
            "[9,   400] loss:0.538\n",
            "[9,   600] loss:0.542\n",
            "[10,   200] loss:0.505\n",
            "[10,   400] loss:0.488\n",
            "[10,   600] loss:0.510\n",
            "[11,   200] loss:0.474\n",
            "[11,   400] loss:0.471\n",
            "[11,   600] loss:0.479\n",
            "[12,   200] loss:0.440\n",
            "[12,   400] loss:0.455\n",
            "[12,   600] loss:0.449\n",
            "[13,   200] loss:0.422\n",
            "[13,   400] loss:0.418\n",
            "[13,   600] loss:0.421\n",
            "[14,   200] loss:0.401\n",
            "[14,   400] loss:0.421\n",
            "[14,   600] loss:0.399\n",
            "[15,   200] loss:0.373\n",
            "[15,   400] loss:0.385\n",
            "[15,   600] loss:0.384\n",
            "[16,   200] loss:0.363\n",
            "[16,   400] loss:0.362\n",
            "[16,   600] loss:0.368\n",
            "[17,   200] loss:0.334\n",
            "[17,   400] loss:0.337\n",
            "[17,   600] loss:0.353\n",
            "[18,   200] loss:0.315\n",
            "[18,   400] loss:0.340\n",
            "[18,   600] loss:0.331\n",
            "[19,   200] loss:0.313\n",
            "[19,   400] loss:0.320\n",
            "[19,   600] loss:0.319\n",
            "[20,   200] loss:0.293\n",
            "[20,   400] loss:0.308\n",
            "[20,   600] loss:0.302\n",
            "[21,   200] loss:0.278\n",
            "[21,   400] loss:0.300\n",
            "[21,   600] loss:0.290\n",
            "[22,   200] loss:0.273\n",
            "[22,   400] loss:0.275\n",
            "[22,   600] loss:0.278\n",
            "[23,   200] loss:0.259\n",
            "[23,   400] loss:0.266\n",
            "[23,   600] loss:0.271\n",
            "[24,   200] loss:0.250\n",
            "[24,   400] loss:0.264\n",
            "[24,   600] loss:0.247\n",
            "[25,   200] loss:0.230\n",
            "[25,   400] loss:0.240\n",
            "[25,   600] loss:0.250\n",
            "[26,   200] loss:0.221\n",
            "[26,   400] loss:0.241\n",
            "[26,   600] loss:0.235\n",
            "[27,   200] loss:0.213\n",
            "[27,   400] loss:0.234\n",
            "[27,   600] loss:0.234\n",
            "[28,   200] loss:0.206\n",
            "[28,   400] loss:0.214\n",
            "[28,   600] loss:0.229\n",
            "[29,   200] loss:0.205\n",
            "[29,   400] loss:0.208\n",
            "[29,   600] loss:0.207\n",
            "[30,   200] loss:0.192\n",
            "[30,   400] loss:0.206\n",
            "[30,   600] loss:0.200\n",
            "[31,   200] loss:0.192\n",
            "[31,   400] loss:0.202\n",
            "[31,   600] loss:0.194\n",
            "[32,   200] loss:0.172\n",
            "[32,   400] loss:0.188\n",
            "[32,   600] loss:0.197\n",
            "[33,   200] loss:0.175\n",
            "[33,   400] loss:0.181\n",
            "[33,   600] loss:0.184\n",
            "[34,   200] loss:0.163\n",
            "[34,   400] loss:0.172\n",
            "[34,   600] loss:0.175\n",
            "[35,   200] loss:0.150\n",
            "[35,   400] loss:0.164\n",
            "[35,   600] loss:0.169\n",
            "[36,   200] loss:0.147\n",
            "[36,   400] loss:0.152\n",
            "[36,   600] loss:0.171\n",
            "[37,   200] loss:0.149\n",
            "[37,   400] loss:0.151\n",
            "[37,   600] loss:0.147\n",
            "[38,   200] loss:0.138\n",
            "[38,   400] loss:0.148\n",
            "[38,   600] loss:0.157\n",
            "[39,   200] loss:0.129\n",
            "[39,   400] loss:0.149\n",
            "[39,   600] loss:0.148\n",
            "[40,   200] loss:0.133\n",
            "[40,   400] loss:0.140\n",
            "[40,   600] loss:0.149\n",
            "[41,   200] loss:0.130\n",
            "[41,   400] loss:0.132\n",
            "[41,   600] loss:0.137\n",
            "[42,   200] loss:0.123\n",
            "[42,   400] loss:0.134\n",
            "[42,   600] loss:0.136\n",
            "[43,   200] loss:0.115\n",
            "[43,   400] loss:0.126\n",
            "[43,   600] loss:0.133\n",
            "[44,   200] loss:0.110\n",
            "[44,   400] loss:0.113\n",
            "[44,   600] loss:0.132\n",
            "[45,   200] loss:0.106\n",
            "[45,   400] loss:0.115\n",
            "[45,   600] loss:0.123\n",
            "[46,   200] loss:0.098\n",
            "[46,   400] loss:0.109\n",
            "[46,   600] loss:0.125\n",
            "[47,   200] loss:0.114\n",
            "[47,   400] loss:0.112\n",
            "[47,   600] loss:0.105\n",
            "[48,   200] loss:0.106\n",
            "[48,   400] loss:0.107\n",
            "[48,   600] loss:0.115\n",
            "[49,   200] loss:0.106\n",
            "[49,   400] loss:0.104\n",
            "[49,   600] loss:0.110\n",
            "[50,   200] loss:0.097\n",
            "[50,   400] loss:0.115\n",
            "[50,   600] loss:0.109\n",
            "[51,   200] loss:0.093\n",
            "[51,   400] loss:0.098\n",
            "[51,   600] loss:0.096\n",
            "[52,   200] loss:0.087\n",
            "[52,   400] loss:0.097\n",
            "[52,   600] loss:0.099\n",
            "[53,   200] loss:0.094\n",
            "[53,   400] loss:0.094\n",
            "[53,   600] loss:0.093\n",
            "[54,   200] loss:0.087\n",
            "[54,   400] loss:0.091\n",
            "[54,   600] loss:0.088\n",
            "[55,   200] loss:0.082\n",
            "[55,   400] loss:0.089\n",
            "[55,   600] loss:0.092\n",
            "[56,   200] loss:0.080\n",
            "[56,   400] loss:0.089\n",
            "[56,   600] loss:0.087\n",
            "[57,   200] loss:0.081\n",
            "[57,   400] loss:0.085\n",
            "[57,   600] loss:0.092\n",
            "[58,   200] loss:0.074\n",
            "[58,   400] loss:0.078\n",
            "[58,   600] loss:0.082\n",
            "[59,   200] loss:0.079\n",
            "[59,   400] loss:0.082\n",
            "[59,   600] loss:0.085\n",
            "[60,   200] loss:0.069\n",
            "[60,   400] loss:0.078\n",
            "[60,   600] loss:0.089\n",
            "[61,   200] loss:0.080\n",
            "[61,   400] loss:0.074\n",
            "[61,   600] loss:0.078\n",
            "[62,   200] loss:0.066\n",
            "[62,   400] loss:0.084\n",
            "[62,   600] loss:0.078\n",
            "[63,   200] loss:0.063\n",
            "[63,   400] loss:0.077\n",
            "[63,   600] loss:0.081\n",
            "[64,   200] loss:0.070\n",
            "[64,   400] loss:0.072\n",
            "[64,   600] loss:0.071\n",
            "[65,   200] loss:0.060\n",
            "[65,   400] loss:0.063\n",
            "[65,   600] loss:0.079\n",
            "[66,   200] loss:0.066\n",
            "[66,   400] loss:0.077\n",
            "[66,   600] loss:0.067\n",
            "[67,   200] loss:0.064\n",
            "[67,   400] loss:0.065\n",
            "[67,   600] loss:0.072\n",
            "[68,   200] loss:0.056\n",
            "[68,   400] loss:0.060\n",
            "[68,   600] loss:0.079\n",
            "[69,   200] loss:0.061\n",
            "[69,   400] loss:0.074\n",
            "[69,   600] loss:0.071\n",
            "[70,   200] loss:0.059\n",
            "[70,   400] loss:0.056\n",
            "[70,   600] loss:0.067\n",
            "[71,   200] loss:0.062\n",
            "[71,   400] loss:0.056\n",
            "[71,   600] loss:0.074\n",
            "[72,   200] loss:0.061\n",
            "[72,   400] loss:0.065\n",
            "[72,   600] loss:0.066\n",
            "[73,   200] loss:0.061\n",
            "[73,   400] loss:0.050\n",
            "[73,   600] loss:0.061\n",
            "[74,   200] loss:0.049\n",
            "[74,   400] loss:0.063\n",
            "[74,   600] loss:0.064\n",
            "[75,   200] loss:0.054\n",
            "[75,   400] loss:0.062\n",
            "[75,   600] loss:0.059\n",
            "[76,   200] loss:0.050\n",
            "[76,   400] loss:0.056\n",
            "[76,   600] loss:0.058\n",
            "[77,   200] loss:0.048\n",
            "[77,   400] loss:0.055\n",
            "[77,   600] loss:0.055\n",
            "[78,   200] loss:0.059\n",
            "[78,   400] loss:0.057\n",
            "[78,   600] loss:0.062\n",
            "[79,   200] loss:0.051\n",
            "[79,   400] loss:0.065\n",
            "[79,   600] loss:0.058\n",
            "[80,   200] loss:0.053\n",
            "[80,   400] loss:0.060\n",
            "[80,   600] loss:0.057\n",
            "[81,   200] loss:0.051\n",
            "[81,   400] loss:0.051\n",
            "[81,   600] loss:0.057\n",
            "[82,   200] loss:0.048\n",
            "[82,   400] loss:0.049\n",
            "[82,   600] loss:0.065\n",
            "[83,   200] loss:0.051\n",
            "[83,   400] loss:0.053\n",
            "[83,   600] loss:0.054\n",
            "[84,   200] loss:0.045\n",
            "[84,   400] loss:0.046\n",
            "[84,   600] loss:0.052\n",
            "[85,   200] loss:0.049\n",
            "[85,   400] loss:0.056\n",
            "[85,   600] loss:0.054\n",
            "[86,   200] loss:0.049\n",
            "[86,   400] loss:0.051\n",
            "[86,   600] loss:0.050\n",
            "[87,   200] loss:0.047\n",
            "[87,   400] loss:0.053\n",
            "[87,   600] loss:0.049\n",
            "[88,   200] loss:0.053\n",
            "[88,   400] loss:0.053\n",
            "[88,   600] loss:0.047\n",
            "[89,   200] loss:0.046\n",
            "[89,   400] loss:0.050\n",
            "[89,   600] loss:0.046\n",
            "[90,   200] loss:0.051\n",
            "[90,   400] loss:0.052\n",
            "[90,   600] loss:0.050\n",
            "[91,   200] loss:0.045\n",
            "[91,   400] loss:0.048\n",
            "[91,   600] loss:0.052\n",
            "[92,   200] loss:0.049\n",
            "[92,   400] loss:0.048\n",
            "[92,   600] loss:0.043\n",
            "[93,   200] loss:0.042\n",
            "[93,   400] loss:0.047\n",
            "[93,   600] loss:0.042\n",
            "[94,   200] loss:0.042\n",
            "[94,   400] loss:0.051\n",
            "[94,   600] loss:0.048\n",
            "[95,   200] loss:0.039\n",
            "[95,   400] loss:0.047\n",
            "[95,   600] loss:0.043\n",
            "[96,   200] loss:0.043\n",
            "[96,   400] loss:0.043\n",
            "[96,   600] loss:0.047\n",
            "[97,   200] loss:0.042\n",
            "[97,   400] loss:0.050\n",
            "[97,   600] loss:0.048\n",
            "[98,   200] loss:0.042\n",
            "[98,   400] loss:0.042\n",
            "[98,   600] loss:0.041\n",
            "[99,   200] loss:0.041\n",
            "[99,   400] loss:0.047\n",
            "[99,   600] loss:0.044\n",
            "[100,   200] loss:0.038\n",
            "[100,   400] loss:0.047\n",
            "[100,   600] loss:0.045\n",
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfrj0j35U9P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model9.state_dict(), PATH)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TagEXS_LtmLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "c99ce166-10b0-45e2-f590-095b5ba6ded6"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model9))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model9))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model9))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39676 / 40000 with accuracy 99.19\n",
            "train acc None\n",
            "Got 9145 / 10000 with accuracy 91.45\n",
            "val acc None\n",
            "Got 9076 / 10000 with accuracy 90.76\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veg_pxeIfR95",
        "colab_type": "text"
      },
      "source": [
        "**3 Dropout ResNet-34 0.2** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8LOk1mfRCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock_02(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock_02, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.dropout(self.conv2(out)))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet_02(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_02, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        m = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "        out = m(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet34_022():\n",
        "    return ResNet_3(BasicBlock_02, [3, 4, 6, 3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB1pXYQHfQn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model9a = ResNet34_022()\n",
        "if torch.cuda.is_available():\n",
        "    model9a.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model9a.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns0G6uh0fQKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb1daeaf-7060-4b60-cbb0-446388914f1f"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model9a(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:2.067\n",
            "[1,   400] loss:1.680\n",
            "[1,   600] loss:1.562\n",
            "[2,   200] loss:1.386\n",
            "[2,   400] loss:1.309\n",
            "[2,   600] loss:1.197\n",
            "[3,   200] loss:1.122\n",
            "[3,   400] loss:1.032\n",
            "[3,   600] loss:0.975\n",
            "[4,   200] loss:0.919\n",
            "[4,   400] loss:0.874\n",
            "[4,   600] loss:0.845\n",
            "[5,   200] loss:0.788\n",
            "[5,   400] loss:0.785\n",
            "[5,   600] loss:0.748\n",
            "[6,   200] loss:0.699\n",
            "[6,   400] loss:0.711\n",
            "[6,   600] loss:0.697\n",
            "[7,   200] loss:0.648\n",
            "[7,   400] loss:0.650\n",
            "[7,   600] loss:0.633\n",
            "[8,   200] loss:0.601\n",
            "[8,   400] loss:0.601\n",
            "[8,   600] loss:0.595\n",
            "[9,   200] loss:0.571\n",
            "[9,   400] loss:0.570\n",
            "[9,   600] loss:0.542\n",
            "[10,   200] loss:0.515\n",
            "[10,   400] loss:0.516\n",
            "[10,   600] loss:0.531\n",
            "[11,   200] loss:0.488\n",
            "[11,   400] loss:0.497\n",
            "[11,   600] loss:0.495\n",
            "[12,   200] loss:0.466\n",
            "[12,   400] loss:0.458\n",
            "[12,   600] loss:0.462\n",
            "[13,   200] loss:0.428\n",
            "[13,   400] loss:0.434\n",
            "[13,   600] loss:0.449\n",
            "[14,   200] loss:0.411\n",
            "[14,   400] loss:0.406\n",
            "[14,   600] loss:0.432\n",
            "[15,   200] loss:0.392\n",
            "[15,   400] loss:0.406\n",
            "[15,   600] loss:0.399\n",
            "[16,   200] loss:0.374\n",
            "[16,   400] loss:0.385\n",
            "[16,   600] loss:0.385\n",
            "[17,   200] loss:0.356\n",
            "[17,   400] loss:0.372\n",
            "[17,   600] loss:0.365\n",
            "[18,   200] loss:0.329\n",
            "[18,   400] loss:0.347\n",
            "[18,   600] loss:0.363\n",
            "[19,   200] loss:0.316\n",
            "[19,   400] loss:0.332\n",
            "[19,   600] loss:0.342\n",
            "[20,   200] loss:0.312\n",
            "[20,   400] loss:0.327\n",
            "[20,   600] loss:0.327\n",
            "[21,   200] loss:0.298\n",
            "[21,   400] loss:0.303\n",
            "[21,   600] loss:0.315\n",
            "[22,   200] loss:0.290\n",
            "[22,   400] loss:0.288\n",
            "[22,   600] loss:0.295\n",
            "[23,   200] loss:0.270\n",
            "[23,   400] loss:0.286\n",
            "[23,   600] loss:0.292\n",
            "[24,   200] loss:0.253\n",
            "[24,   400] loss:0.272\n",
            "[24,   600] loss:0.270\n",
            "[25,   200] loss:0.253\n",
            "[25,   400] loss:0.261\n",
            "[25,   600] loss:0.265\n",
            "[26,   200] loss:0.246\n",
            "[26,   400] loss:0.257\n",
            "[26,   600] loss:0.255\n",
            "[27,   200] loss:0.216\n",
            "[27,   400] loss:0.250\n",
            "[27,   600] loss:0.251\n",
            "[28,   200] loss:0.223\n",
            "[28,   400] loss:0.229\n",
            "[28,   600] loss:0.231\n",
            "[29,   200] loss:0.213\n",
            "[29,   400] loss:0.214\n",
            "[29,   600] loss:0.233\n",
            "[30,   200] loss:0.207\n",
            "[30,   400] loss:0.218\n",
            "[30,   600] loss:0.216\n",
            "[31,   200] loss:0.200\n",
            "[31,   400] loss:0.206\n",
            "[31,   600] loss:0.213\n",
            "[32,   200] loss:0.196\n",
            "[32,   400] loss:0.195\n",
            "[32,   600] loss:0.207\n",
            "[33,   200] loss:0.192\n",
            "[33,   400] loss:0.200\n",
            "[33,   600] loss:0.197\n",
            "[34,   200] loss:0.170\n",
            "[34,   400] loss:0.181\n",
            "[34,   600] loss:0.197\n",
            "[35,   200] loss:0.168\n",
            "[35,   400] loss:0.182\n",
            "[35,   600] loss:0.193\n",
            "[36,   200] loss:0.171\n",
            "[36,   400] loss:0.185\n",
            "[36,   600] loss:0.182\n",
            "[37,   200] loss:0.167\n",
            "[37,   400] loss:0.171\n",
            "[37,   600] loss:0.168\n",
            "[38,   200] loss:0.154\n",
            "[38,   400] loss:0.167\n",
            "[38,   600] loss:0.172\n",
            "[39,   200] loss:0.159\n",
            "[39,   400] loss:0.158\n",
            "[39,   600] loss:0.160\n",
            "[40,   200] loss:0.138\n",
            "[40,   400] loss:0.147\n",
            "[40,   600] loss:0.164\n",
            "[41,   200] loss:0.139\n",
            "[41,   400] loss:0.147\n",
            "[41,   600] loss:0.153\n",
            "[42,   200] loss:0.132\n",
            "[42,   400] loss:0.141\n",
            "[42,   600] loss:0.140\n",
            "[43,   200] loss:0.138\n",
            "[43,   400] loss:0.140\n",
            "[43,   600] loss:0.143\n",
            "[44,   200] loss:0.136\n",
            "[44,   400] loss:0.137\n",
            "[44,   600] loss:0.136\n",
            "[45,   200] loss:0.112\n",
            "[45,   400] loss:0.130\n",
            "[45,   600] loss:0.139\n",
            "[46,   200] loss:0.121\n",
            "[46,   400] loss:0.133\n",
            "[46,   600] loss:0.137\n",
            "[47,   200] loss:0.115\n",
            "[47,   400] loss:0.128\n",
            "[47,   600] loss:0.132\n",
            "[48,   200] loss:0.123\n",
            "[48,   400] loss:0.119\n",
            "[48,   600] loss:0.124\n",
            "[49,   200] loss:0.109\n",
            "[49,   400] loss:0.120\n",
            "[49,   600] loss:0.126\n",
            "[50,   200] loss:0.111\n",
            "[50,   400] loss:0.114\n",
            "[50,   600] loss:0.119\n",
            "[51,   200] loss:0.102\n",
            "[51,   400] loss:0.110\n",
            "[51,   600] loss:0.113\n",
            "[52,   200] loss:0.100\n",
            "[52,   400] loss:0.101\n",
            "[52,   600] loss:0.113\n",
            "[53,   200] loss:0.100\n",
            "[53,   400] loss:0.105\n",
            "[53,   600] loss:0.103\n",
            "[54,   200] loss:0.093\n",
            "[54,   400] loss:0.107\n",
            "[54,   600] loss:0.108\n",
            "[55,   200] loss:0.093\n",
            "[55,   400] loss:0.102\n",
            "[55,   600] loss:0.112\n",
            "[56,   200] loss:0.094\n",
            "[56,   400] loss:0.098\n",
            "[56,   600] loss:0.097\n",
            "[57,   200] loss:0.094\n",
            "[57,   400] loss:0.095\n",
            "[57,   600] loss:0.096\n",
            "[58,   200] loss:0.084\n",
            "[58,   400] loss:0.090\n",
            "[58,   600] loss:0.096\n",
            "[59,   200] loss:0.089\n",
            "[59,   400] loss:0.089\n",
            "[59,   600] loss:0.094\n",
            "[60,   200] loss:0.091\n",
            "[60,   400] loss:0.083\n",
            "[60,   600] loss:0.104\n",
            "[61,   200] loss:0.083\n",
            "[61,   400] loss:0.084\n",
            "[61,   600] loss:0.095\n",
            "[62,   200] loss:0.081\n",
            "[62,   400] loss:0.083\n",
            "[62,   600] loss:0.088\n",
            "[63,   200] loss:0.076\n",
            "[63,   400] loss:0.086\n",
            "[63,   600] loss:0.086\n",
            "[64,   200] loss:0.081\n",
            "[64,   400] loss:0.079\n",
            "[64,   600] loss:0.084\n",
            "[65,   200] loss:0.074\n",
            "[65,   400] loss:0.081\n",
            "[65,   600] loss:0.077\n",
            "[66,   200] loss:0.072\n",
            "[66,   400] loss:0.086\n",
            "[66,   600] loss:0.086\n",
            "[67,   200] loss:0.072\n",
            "[67,   400] loss:0.077\n",
            "[67,   600] loss:0.073\n",
            "[68,   200] loss:0.066\n",
            "[68,   400] loss:0.070\n",
            "[68,   600] loss:0.075\n",
            "[69,   200] loss:0.080\n",
            "[69,   400] loss:0.072\n",
            "[69,   600] loss:0.072\n",
            "[70,   200] loss:0.076\n",
            "[70,   400] loss:0.068\n",
            "[70,   600] loss:0.085\n",
            "[71,   200] loss:0.067\n",
            "[71,   400] loss:0.066\n",
            "[71,   600] loss:0.070\n",
            "[72,   200] loss:0.067\n",
            "[72,   400] loss:0.067\n",
            "[72,   600] loss:0.081\n",
            "[73,   200] loss:0.065\n",
            "[73,   400] loss:0.076\n",
            "[73,   600] loss:0.073\n",
            "[74,   200] loss:0.061\n",
            "[74,   400] loss:0.065\n",
            "[74,   600] loss:0.068\n",
            "[75,   200] loss:0.066\n",
            "[75,   400] loss:0.069\n",
            "[75,   600] loss:0.068\n",
            "[76,   200] loss:0.063\n",
            "[76,   400] loss:0.070\n",
            "[76,   600] loss:0.073\n",
            "[77,   200] loss:0.061\n",
            "[77,   400] loss:0.070\n",
            "[77,   600] loss:0.070\n",
            "[78,   200] loss:0.058\n",
            "[78,   400] loss:0.059\n",
            "[78,   600] loss:0.071\n",
            "[79,   200] loss:0.054\n",
            "[79,   400] loss:0.065\n",
            "[79,   600] loss:0.063\n",
            "[80,   200] loss:0.059\n",
            "[80,   400] loss:0.059\n",
            "[80,   600] loss:0.069\n",
            "[81,   200] loss:0.052\n",
            "[81,   400] loss:0.061\n",
            "[81,   600] loss:0.062\n",
            "[82,   200] loss:0.058\n",
            "[82,   400] loss:0.055\n",
            "[82,   600] loss:0.064\n",
            "[83,   200] loss:0.052\n",
            "[83,   400] loss:0.065\n",
            "[83,   600] loss:0.053\n",
            "[84,   200] loss:0.053\n",
            "[84,   400] loss:0.062\n",
            "[84,   600] loss:0.059\n",
            "[85,   200] loss:0.059\n",
            "[85,   400] loss:0.054\n",
            "[85,   600] loss:0.056\n",
            "[86,   200] loss:0.050\n",
            "[86,   400] loss:0.055\n",
            "[86,   600] loss:0.052\n",
            "[87,   200] loss:0.059\n",
            "[87,   400] loss:0.058\n",
            "[87,   600] loss:0.063\n",
            "[88,   200] loss:0.050\n",
            "[88,   400] loss:0.056\n",
            "[88,   600] loss:0.053\n",
            "[89,   200] loss:0.051\n",
            "[89,   400] loss:0.059\n",
            "[89,   600] loss:0.052\n",
            "[90,   200] loss:0.054\n",
            "[90,   400] loss:0.060\n",
            "[90,   600] loss:0.054\n",
            "[91,   200] loss:0.054\n",
            "[91,   400] loss:0.054\n",
            "[91,   600] loss:0.047\n",
            "[92,   200] loss:0.040\n",
            "[92,   400] loss:0.051\n",
            "[92,   600] loss:0.055\n",
            "[93,   200] loss:0.044\n",
            "[93,   400] loss:0.050\n",
            "[93,   600] loss:0.054\n",
            "[94,   200] loss:0.049\n",
            "[94,   400] loss:0.048\n",
            "[94,   600] loss:0.045\n",
            "[95,   200] loss:0.045\n",
            "[95,   400] loss:0.053\n",
            "[95,   600] loss:0.055\n",
            "[96,   200] loss:0.045\n",
            "[96,   400] loss:0.042\n",
            "[96,   600] loss:0.053\n",
            "[97,   200] loss:0.049\n",
            "[97,   400] loss:0.055\n",
            "[97,   600] loss:0.055\n",
            "[98,   200] loss:0.041\n",
            "[98,   400] loss:0.045\n",
            "[98,   600] loss:0.051\n",
            "[99,   200] loss:0.042\n",
            "[99,   400] loss:0.046\n",
            "[99,   600] loss:0.047\n",
            "[100,   200] loss:0.042\n",
            "[100,   400] loss:0.051\n",
            "[100,   600] loss:0.050\n",
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6rNWeK8KfWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model9a.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJnXwxz6g6B3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "28a3c4a0-e83c-43fb-8b4f-c86ca4596508"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model9a))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model9a))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model9a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39651 / 40000 with accuracy 99.13\n",
            "train acc None\n",
            "Got 9155 / 10000 with accuracy 91.55\n",
            "val acc None\n",
            "Got 9085 / 10000 with accuracy 90.85\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijbE2OAqb8Tr",
        "colab_type": "text"
      },
      "source": [
        "**3 ResNet 50 dropout 0.25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1vEng9av8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bottleneck_5(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck_5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.dropout(self.conv2(out))))\n",
        "        out = self.bn3(self.dropout(self.conv3(out)))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_5(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_5, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        m = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "        out = m(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet50_5():\n",
        "    return ResNet_5(Bottleneck_5, [3, 4, 6, 3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkHuKjAWcDeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_channel = 3\n",
        "num_classes = 10 \n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 100\n",
        "\n",
        "model10 = ResNet50_5()\n",
        "if torch.cuda.is_available():\n",
        "    model10.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model10.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5002HiEcNm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "987b1ef8-8c9b-480b-a362-0cbb8025c3a5"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, targets) in enumerate(train_tf_loader0):\n",
        "        # Get data to cuda if possible\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "        \n",
        "        scores = model10(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if batch_idx%200 == 199: \n",
        "            print('[%d, %5d] loss:%.3f'% (epoch +1, batch_idx+1, running_loss/200))\n",
        "            running_loss = 0\n",
        "\n",
        "print('finished', epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss:2.699\n",
            "[1,   400] loss:2.135\n",
            "[1,   600] loss:1.913\n",
            "[2,   200] loss:1.789\n",
            "[2,   400] loss:1.697\n",
            "[2,   600] loss:1.639\n",
            "[3,   200] loss:1.541\n",
            "[3,   400] loss:1.493\n",
            "[3,   600] loss:1.452\n",
            "[4,   200] loss:1.360\n",
            "[4,   400] loss:1.302\n",
            "[4,   600] loss:1.280\n",
            "[5,   200] loss:1.225\n",
            "[5,   400] loss:1.181\n",
            "[5,   600] loss:1.147\n",
            "[6,   200] loss:1.129\n",
            "[6,   400] loss:1.077\n",
            "[6,   600] loss:1.072\n",
            "[7,   200] loss:1.055\n",
            "[7,   400] loss:1.002\n",
            "[7,   600] loss:0.979\n",
            "[8,   200] loss:0.948\n",
            "[8,   400] loss:0.923\n",
            "[8,   600] loss:0.946\n",
            "[9,   200] loss:0.894\n",
            "[9,   400] loss:0.879\n",
            "[9,   600] loss:0.870\n",
            "[10,   200] loss:0.830\n",
            "[10,   400] loss:0.839\n",
            "[10,   600] loss:0.838\n",
            "[11,   200] loss:0.813\n",
            "[11,   400] loss:0.783\n",
            "[11,   600] loss:0.773\n",
            "[12,   200] loss:0.750\n",
            "[12,   400] loss:0.749\n",
            "[12,   600] loss:0.723\n",
            "[13,   200] loss:0.698\n",
            "[13,   400] loss:0.712\n",
            "[13,   600] loss:0.688\n",
            "[14,   200] loss:0.691\n",
            "[14,   400] loss:0.668\n",
            "[14,   600] loss:0.653\n",
            "[15,   200] loss:0.633\n",
            "[15,   400] loss:0.622\n",
            "[15,   600] loss:0.624\n",
            "[16,   200] loss:0.627\n",
            "[16,   400] loss:0.604\n",
            "[16,   600] loss:0.600\n",
            "[17,   200] loss:0.580\n",
            "[17,   400] loss:0.568\n",
            "[17,   600] loss:0.576\n",
            "[18,   200] loss:0.554\n",
            "[18,   400] loss:0.575\n",
            "[18,   600] loss:0.534\n",
            "[19,   200] loss:0.533\n",
            "[19,   400] loss:0.529\n",
            "[19,   600] loss:0.536\n",
            "[20,   200] loss:0.517\n",
            "[20,   400] loss:0.510\n",
            "[20,   600] loss:0.511\n",
            "[21,   200] loss:0.481\n",
            "[21,   400] loss:0.507\n",
            "[21,   600] loss:0.493\n",
            "[22,   200] loss:0.471\n",
            "[22,   400] loss:0.481\n",
            "[22,   600] loss:0.493\n",
            "[23,   200] loss:0.467\n",
            "[23,   400] loss:0.464\n",
            "[23,   600] loss:0.459\n",
            "[24,   200] loss:0.449\n",
            "[24,   400] loss:0.453\n",
            "[24,   600] loss:0.445\n",
            "[25,   200] loss:0.434\n",
            "[25,   400] loss:0.427\n",
            "[25,   600] loss:0.435\n",
            "[26,   200] loss:0.420\n",
            "[26,   400] loss:0.431\n",
            "[26,   600] loss:0.437\n",
            "[27,   200] loss:0.395\n",
            "[27,   400] loss:0.416\n",
            "[27,   600] loss:0.414\n",
            "[28,   200] loss:0.391\n",
            "[28,   400] loss:0.396\n",
            "[28,   600] loss:0.408\n",
            "[29,   200] loss:0.374\n",
            "[29,   400] loss:0.387\n",
            "[29,   600] loss:0.392\n",
            "[30,   200] loss:0.362\n",
            "[30,   400] loss:0.373\n",
            "[30,   600] loss:0.381\n",
            "[31,   200] loss:0.355\n",
            "[31,   400] loss:0.368\n",
            "[31,   600] loss:0.367\n",
            "[32,   200] loss:0.353\n",
            "[32,   400] loss:0.355\n",
            "[32,   600] loss:0.363\n",
            "[33,   200] loss:0.340\n",
            "[33,   400] loss:0.340\n",
            "[33,   600] loss:0.364\n",
            "[34,   200] loss:0.337\n",
            "[34,   400] loss:0.334\n",
            "[34,   600] loss:0.338\n",
            "[35,   200] loss:0.318\n",
            "[35,   400] loss:0.334\n",
            "[35,   600] loss:0.333\n",
            "[36,   200] loss:0.326\n",
            "[36,   400] loss:0.319\n",
            "[36,   600] loss:0.328\n",
            "[37,   200] loss:0.302\n",
            "[37,   400] loss:0.317\n",
            "[37,   600] loss:0.308\n",
            "[38,   200] loss:0.302\n",
            "[38,   400] loss:0.308\n",
            "[38,   600] loss:0.310\n",
            "[39,   200] loss:0.301\n",
            "[39,   400] loss:0.307\n",
            "[39,   600] loss:0.294\n",
            "[40,   200] loss:0.286\n",
            "[40,   400] loss:0.287\n",
            "[40,   600] loss:0.289\n",
            "[41,   200] loss:0.281\n",
            "[41,   400] loss:0.291\n",
            "[41,   600] loss:0.286\n",
            "[42,   200] loss:0.275\n",
            "[42,   400] loss:0.286\n",
            "[42,   600] loss:0.265\n",
            "[43,   200] loss:0.256\n",
            "[43,   400] loss:0.279\n",
            "[43,   600] loss:0.275\n",
            "[44,   200] loss:0.258\n",
            "[44,   400] loss:0.275\n",
            "[44,   600] loss:0.266\n",
            "[45,   200] loss:0.243\n",
            "[45,   400] loss:0.250\n",
            "[45,   600] loss:0.263\n",
            "[46,   200] loss:0.241\n",
            "[46,   400] loss:0.251\n",
            "[46,   600] loss:0.258\n",
            "[47,   200] loss:0.249\n",
            "[47,   400] loss:0.250\n",
            "[47,   600] loss:0.254\n",
            "[48,   200] loss:0.241\n",
            "[48,   400] loss:0.233\n",
            "[48,   600] loss:0.246\n",
            "[49,   200] loss:0.230\n",
            "[49,   400] loss:0.234\n",
            "[49,   600] loss:0.240\n",
            "[50,   200] loss:0.228\n",
            "[50,   400] loss:0.223\n",
            "[50,   600] loss:0.244\n",
            "[51,   200] loss:0.212\n",
            "[51,   400] loss:0.239\n",
            "[51,   600] loss:0.236\n",
            "[52,   200] loss:0.205\n",
            "[52,   400] loss:0.225\n",
            "[52,   600] loss:0.231\n",
            "[53,   200] loss:0.194\n",
            "[53,   400] loss:0.229\n",
            "[53,   600] loss:0.219\n",
            "[54,   200] loss:0.206\n",
            "[54,   400] loss:0.223\n",
            "[54,   600] loss:0.209\n",
            "[55,   200] loss:0.212\n",
            "[55,   400] loss:0.201\n",
            "[55,   600] loss:0.212\n",
            "[56,   200] loss:0.187\n",
            "[56,   400] loss:0.200\n",
            "[56,   600] loss:0.210\n",
            "[57,   200] loss:0.191\n",
            "[57,   400] loss:0.203\n",
            "[57,   600] loss:0.199\n",
            "[58,   200] loss:0.182\n",
            "[58,   400] loss:0.194\n",
            "[58,   600] loss:0.191\n",
            "[59,   200] loss:0.177\n",
            "[59,   400] loss:0.185\n",
            "[59,   600] loss:0.201\n",
            "[60,   200] loss:0.177\n",
            "[60,   400] loss:0.189\n",
            "[60,   600] loss:0.191\n",
            "[61,   200] loss:0.172\n",
            "[61,   400] loss:0.193\n",
            "[61,   600] loss:0.192\n",
            "[62,   200] loss:0.175\n",
            "[62,   400] loss:0.183\n",
            "[62,   600] loss:0.185\n",
            "[63,   200] loss:0.167\n",
            "[63,   400] loss:0.172\n",
            "[63,   600] loss:0.179\n",
            "[64,   200] loss:0.158\n",
            "[64,   400] loss:0.177\n",
            "[64,   600] loss:0.170\n",
            "[65,   200] loss:0.157\n",
            "[65,   400] loss:0.170\n",
            "[65,   600] loss:0.172\n",
            "[66,   200] loss:0.167\n",
            "[66,   400] loss:0.165\n",
            "[66,   600] loss:0.174\n",
            "[67,   200] loss:0.157\n",
            "[67,   400] loss:0.172\n",
            "[67,   600] loss:0.177\n",
            "[68,   200] loss:0.147\n",
            "[68,   400] loss:0.166\n",
            "[68,   600] loss:0.168\n",
            "[69,   200] loss:0.145\n",
            "[69,   400] loss:0.161\n",
            "[69,   600] loss:0.154\n",
            "[70,   200] loss:0.149\n",
            "[70,   400] loss:0.158\n",
            "[70,   600] loss:0.168\n",
            "[71,   200] loss:0.145\n",
            "[71,   400] loss:0.156\n",
            "[71,   600] loss:0.159\n",
            "[72,   200] loss:0.140\n",
            "[72,   400] loss:0.153\n",
            "[72,   600] loss:0.156\n",
            "[73,   200] loss:0.135\n",
            "[73,   400] loss:0.150\n",
            "[73,   600] loss:0.159\n",
            "[74,   200] loss:0.126\n",
            "[74,   400] loss:0.151\n",
            "[74,   600] loss:0.154\n",
            "[75,   200] loss:0.132\n",
            "[75,   400] loss:0.139\n",
            "[75,   600] loss:0.145\n",
            "[76,   200] loss:0.123\n",
            "[76,   400] loss:0.144\n",
            "[76,   600] loss:0.144\n",
            "[77,   200] loss:0.119\n",
            "[77,   400] loss:0.136\n",
            "[77,   600] loss:0.137\n",
            "[78,   200] loss:0.138\n",
            "[78,   400] loss:0.132\n",
            "[78,   600] loss:0.141\n",
            "[79,   200] loss:0.119\n",
            "[79,   400] loss:0.145\n",
            "[79,   600] loss:0.131\n",
            "[80,   200] loss:0.127\n",
            "[80,   400] loss:0.141\n",
            "[80,   600] loss:0.134\n",
            "[81,   200] loss:0.125\n",
            "[81,   400] loss:0.127\n",
            "[81,   600] loss:0.127\n",
            "[82,   200] loss:0.126\n",
            "[82,   400] loss:0.122\n",
            "[82,   600] loss:0.132\n",
            "[83,   200] loss:0.133\n",
            "[83,   400] loss:0.115\n",
            "[83,   600] loss:0.127\n",
            "[84,   200] loss:0.118\n",
            "[84,   400] loss:0.122\n",
            "[84,   600] loss:0.129\n",
            "[85,   200] loss:0.112\n",
            "[85,   400] loss:0.121\n",
            "[85,   600] loss:0.125\n",
            "[86,   200] loss:0.114\n",
            "[86,   400] loss:0.113\n",
            "[86,   600] loss:0.125\n",
            "[87,   200] loss:0.109\n",
            "[87,   400] loss:0.118\n",
            "[87,   600] loss:0.117\n",
            "[88,   200] loss:0.119\n",
            "[88,   400] loss:0.115\n",
            "[88,   600] loss:0.118\n",
            "[89,   200] loss:0.106\n",
            "[89,   400] loss:0.109\n",
            "[89,   600] loss:0.117\n",
            "[90,   200] loss:0.115\n",
            "[90,   400] loss:0.116\n",
            "[90,   600] loss:0.119\n",
            "[91,   200] loss:0.101\n",
            "[91,   400] loss:0.105\n",
            "[91,   600] loss:0.114\n",
            "[92,   200] loss:0.103\n",
            "[92,   400] loss:0.107\n",
            "[92,   600] loss:0.119\n",
            "[93,   200] loss:0.103\n",
            "[93,   400] loss:0.102\n",
            "[93,   600] loss:0.105\n",
            "[94,   200] loss:0.096\n",
            "[94,   400] loss:0.110\n",
            "[94,   600] loss:0.110\n",
            "[95,   200] loss:0.100\n",
            "[95,   400] loss:0.103\n",
            "[95,   600] loss:0.103\n",
            "[96,   200] loss:0.101\n",
            "[96,   400] loss:0.102\n",
            "[96,   600] loss:0.105\n",
            "[97,   200] loss:0.096\n",
            "[97,   400] loss:0.096\n",
            "[97,   600] loss:0.100\n",
            "[98,   200] loss:0.089\n",
            "[98,   400] loss:0.105\n",
            "[98,   600] loss:0.102\n",
            "[99,   200] loss:0.098\n",
            "[99,   400] loss:0.103\n",
            "[99,   600] loss:0.102\n",
            "[100,   200] loss:0.089\n",
            "[100,   400] loss:0.105\n",
            "[100,   600] loss:0.094\n",
            "finished 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VRKrpKDehS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(model10.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUFA0Z-Del0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "367f302f-d52f-4e07-c5a3-d20b8d616dfd"
      },
      "source": [
        "print('train acc', check_accuracy(train_tf_loader0, model10))\n",
        "\n",
        "print('val acc', check_accuracy(val_tf_loader0, model10))\n",
        "\n",
        "print('test acc', check_accuracy(test_loader_0, model10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Got 39250 / 40000 with accuracy 98.12\n",
            "train acc None\n",
            "Got 9060 / 10000 with accuracy 90.60\n",
            "val acc None\n",
            "Got 8954 / 10000 with accuracy 89.54\n",
            "test acc None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
